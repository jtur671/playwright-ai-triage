# ğŸš€ AI Playwright Test Lab

An end-to-end **AI-powered test automation pipeline** that:

âœ”ï¸ **Generates Playwright tests** from plain-English user stories

âœ”ï¸ **Runs your Playwright suite**

âœ”ï¸ **Analyzes failed tests using AI**, producing explanations, root causes, fix suggestions, and flakiness tips

âœ”ï¸ **Builds a beautifully styled HTML dashboard** of all AI results

âœ”ï¸ **One-command workflow** using `npm run ai:flow`

---

## ğŸ“¸ Demo Overview

This workflow turns simple input like:

**`stories/login.md`**

```
As a user, I want to log in with valid credentials so I can access my dashboard.

Acceptance:
- Navigate to login page
- Enter valid email/password
- Click login
- Expect dashboard to load
```

Into:

* A generated Playwright test
* A full run
* AI failure analysis
* A glowing, animated HTML dashboard like this:

**`ai-report.html`** (auto-opens):

* Plain English explanation
* Root cause analysis
* Suggested test fixes
* Flakiness mitigation
* Styled cards, badges, gradients, animations

---

## ğŸ”§ Tech Stack

* **Node.js + ES Modules**
* **Playwright** (`@playwright/test`)
* **OpenAI API (GPT-4.1)**
* **Custom HTML reporting with external CSS**
* **Dark-mode dashboard UI**

---

## ğŸ“‚ Project Structure

```
ai_project/
â”‚
â”œâ”€â”€ stories/
â”‚   â””â”€â”€ login.md               # Your natural language test cases
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ login.spec.js          # Auto-generated Playwright test
â”‚
â”œâ”€â”€ analyze/
â”‚   â”œâ”€â”€ analyzeFailures.js      # AI engine (JSON + HTML dashboard)
â”‚   â””â”€â”€ ...                    # (additional helper files optional)
â”‚
â”œâ”€â”€ ai-analysis.json            # AI output (JSON)
â”œâ”€â”€ ai-report.html              # Human-friendly HTML dashboard
â”œâ”€â”€ ai-report.css               # Dashboard styling
â”‚
â”œâ”€â”€ generateTest.js             # Converts stories â†’ Playwright tests
â”œâ”€â”€ playwright.config.mjs       # Config + JSON reporter
â”œâ”€â”€ package.json                # NPM scripts for full automation
â””â”€â”€ README.md                   # (this file)
```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/jtur671/playwright-ai-triage.git
cd playwright-ai-triage
npm install
npx playwright install
export OPENAI_API_KEY="your-key"
```

---

## ğŸ§  One-Command Workflow

Run this to:

1. Generate the test from your story
2. Run all Playwright tests
3. Analyze failures using AI
4. Build & open the HTML dashboard

```bash
npm run ai:flow
```

---

## ğŸ‘‰ Individual Commands (Optional)

### Generate tests from your stories

```bash
npm run ai:gen
```

Creates Playwright test files under `tests/`.

---

### Run the Playwright test suite

```bash
npm run ai:test
```

Saves `playwright-report.json`.

---

### Analyze failures using AI (JSON only)

```bash
npm run ai:analyze
```

Saves structured output in `ai-analysis.json`.

---

### Build + open the HTML dashboard

```bash
npm run ai:report
```

Auto-opens `ai-report.html` in your browser.

---

## ğŸ¤– How AI Analysis Works

After Playwright executes the tests, all failure metadata is passed to GPT-4.1:

* Error message
* Stack trace
* Test title + project name
* stdout/stderr logs

AI returns structured HTML with:

### **1. Plain-English Explanation**

Why did this fail?

### **2. Probable Root Causes**

2â€“3 likely technical problems.

### **3. Suggested Test Fixes**

Specific Playwright code improvements.

### **4. Flakiness Mitigation**

Ways to reduce intermittent failures.

---

## ğŸ¨ HTML Dashboard Themes & Features

* Dark-mode
* Animated gradient highlight bars
* Glowing hover transitions
* Status badges
* Collapsible AI analysis sections
* External CSS for easy editing
* Radial gradients & neon accent hues

---

## ğŸ§ª Example AI Output (HTML)

```
<h3>Plain-English Explanation</h3>
<p>The success message never appeared...</p>

<h3>Probable Root Causes</h3>
<ul>
  <li>Selector mismatch</li>
  <li>API response delay</li>
  <li>Redirect timing issue</li>
</ul>

<h3>Suggested Test Fixes</h3>
<ul>
  <li>Use page.waitForURL('/dashboard')</li>
  <li>Wait for stable locator instead of getByText</li>
</ul>

<h3>Flakiness Mitigation</h3>
<p>Increase timeout or add a network idle wait.</p>
```

---

## ğŸ“¦ Environment Variables

Set your OpenAI key using one of these methods:

**Option 1: Export (recommended for quick setup)**
```bash
export OPENAI_API_KEY="your-key"
```

**Option 2: .env file (recommended for persistence)**
```bash
# Copy the example file
cp .env.example .env

# Edit .env and add your key
OPENAI_API_KEY=your-key
```

Both methods work! If you use `export`, it takes precedence over the `.env` file.

---

## ğŸ§­ Roadmap & Enhancements

* Multi-story batch generation
* Test-to-story reverse engineering
* Hit-map UI of frequent failures
* CI pipeline integration
* Slack/Teams bot that posts AI insights
* Flaky test scoring over time

---

## ğŸ’¬ Contributing

Pull requests welcome!

Open issues for bugs, ideas, or UX/UI enhancements.

---

## â­ Star the repo if you like it!

This project helps show how AI can supercharge real-world QA automation.

Let's build the future of testing. ğŸ”¥

